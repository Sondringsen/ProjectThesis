{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import diffrax\n",
    "import equinox as eqx\n",
    "import jax\n",
    "import jax.nn as jnn\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import jax.lax as lax\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optax\n",
    "from functools import partial\n",
    "import pickle\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.path.join(os.getcwd(), \"../\"))\n",
    "from src.data.data_reader import DataReader\n",
    "\n",
    "key_number = 8\n",
    "\n",
    "def key():\n",
    "    global key_number\n",
    "    key_number += 1\n",
    "    return jax.random.PRNGKey(key_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(processed_file):\n",
    "    with open(processed_file, 'rb') as f:\n",
    "        sequences = pickle.load(f)\n",
    "    \n",
    "    sorted_train_data = []\n",
    "    for idx, (ts, xs, ts_eval, ys_eval) in enumerate(sequences):\n",
    "        t0 = ts[0]\n",
    "        t_ms = (ts - t0) / 1e6 # Convert nanoseconds to milliseconds\n",
    "        te_ms = (ts_eval - t0) / 1e6\n",
    "\n",
    "        if len(t_ms) != len(xs):\n",
    "            print(f\"Warning: Inconsistent lengths in sequence {idx}: len(t_ms)={len(t_ms)}, len(xs)={len(xs)}\")\n",
    "            continue  \n",
    "\n",
    "        if len(te_ms) != len(ys_eval):\n",
    "            print(f\"Warning: Inconsistent lengths in ts_eval and ys_eval in sequence {idx}\")\n",
    "            continue \n",
    "        \n",
    "        sorted_indices = np.argsort(t_ms)\n",
    "        t_ms = t_ms[sorted_indices]\n",
    "        xs = xs[sorted_indices]\n",
    "\n",
    "        sorted_indices = np.argsort(te_ms)\n",
    "        te_ms = te_ms[sorted_indices]\n",
    "        ys_eval = ys_eval[sorted_indices]\n",
    "\n",
    "        # Keep data as arrays\n",
    "        sorted_train_data.append((t_ms, xs, te_ms, ys_eval))\n",
    "\n",
    "\n",
    "    if not sorted_train_data:\n",
    "        raise ValueError(\"No valid sequences found in the processed data.\")\n",
    "\n",
    "    return sorted_train_data\n",
    "\n",
    "def dataloader(sequences, batch_size, subset_size, *, key):\n",
    "    dataset_size = len(sequences[0])\n",
    "    assert all(len(seq) == dataset_size for seq in sequences)\n",
    "    indices = np.arange(dataset_size)\n",
    "\n",
    "    while True:\n",
    "        subset_perm = np.random.choice(indices, size=subset_size, replace=False)\n",
    "\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "\n",
    "        while start < subset_size:\n",
    "            batch_perm = subset_perm[start:end]\n",
    "            # Ensure data remains as arrays\n",
    "            batch = [ [seq[i] for i in batch_perm] for seq in sequences ]\n",
    "            yield batch\n",
    "            start = end\n",
    "            end = start + batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data(\"../data/processed/Visa_2024-09-06.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n",
      "4\n",
      "24\n",
      "24\n",
      "8\n",
      "8\n",
      "4\n",
      "24\n",
      "24\n",
      "8\n",
      "8\n",
      "6\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(len(data))\n",
    "print(len(data[0]))\n",
    "print(len(data[0][0]))\n",
    "print(len(data[0][1]))\n",
    "print(len(data[0][2]))\n",
    "print(len(data[0][3]))\n",
    "\n",
    "print(len(data[1]))\n",
    "print(len(data[1][0]))\n",
    "print(len(data[1][1]))\n",
    "print(len(data[1][2]))\n",
    "print(len(data[1][3]))\n",
    "\n",
    "print(len(data[0][1][0]))\n",
    "print(type(data[0][1][0]))\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Func(eqx.Module):\n",
    "    mlp: eqx.nn.MLP\n",
    "    data_size: int\n",
    "    hidden_size: int\n",
    "\n",
    "    def __init__(self, data_size, hidden_size, width_size, depth, *, key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.data_size = data_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.mlp = eqx.nn.MLP(\n",
    "            in_size=hidden_size,\n",
    "            out_size=hidden_size * data_size,\n",
    "            width_size=width_size,\n",
    "            depth=depth,\n",
    "            activation=jnn.softplus,\n",
    "            # Note the use of a tanh final activation function. This is important to\n",
    "            # stop the model blowing up. (Just like how GRUs and LSTMs constrain the\n",
    "            # rate of change of their hidden states.)\n",
    "            final_activation=jnn.tanh,\n",
    "            key=key,\n",
    "        )\n",
    "\n",
    "    def __call__(self, t, y, args):\n",
    "        return self.mlp(y).reshape(self.hidden_size, self.data_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralCDE(eqx.Module):\n",
    "    initial: eqx.nn.MLP\n",
    "    func: Func\n",
    "    linear: eqx.nn.Linear\n",
    "\n",
    "    def __init__(self, data_size, hidden_size, width_size, depth, *, key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        ikey, fkey, lkey = jr.split(key, 3)\n",
    "        self.initial = eqx.nn.MLP(data_size + 1, hidden_size, width_size, depth, key=ikey)\n",
    "        self.func = Func(data_size + 1, hidden_size, width_size, depth, key=fkey)\n",
    "        self.linear = eqx.nn.Linear(hidden_size, 1, key=lkey)\n",
    "\n",
    "    def predict_batch(self, ts_batch, x_batch, ts_eval_batch):\n",
    "        # ts_batch = [ts.reshape(-1, 1) for ts in ts_batch]\n",
    "        predictions = jax.vmap(self.predict, in_axes=(0, 0, 0))(ts_batch, x_batch, ts_eval_batch)\n",
    "        # predictions = []\n",
    "        # for ts, x, ts_eval in zip(ts_batch, x_batch, ts_eval_batch):\n",
    "        #     predictions.append(self.predict(ts, x, ts_eval))\n",
    "\n",
    "        return jnp.array(predictions)\n",
    "\n",
    "    def predict(self, ts, x, ts_eval):\n",
    "        ts, x = diffrax.rectilinear_interpolation(ts, x)\n",
    "        data = jnp.hstack([ts.reshape(-1, 1), x])\n",
    "        data = jnp.array(data)\n",
    "\n",
    "        control = diffrax.LinearInterpolation(ts, data)\n",
    "\n",
    "        term = diffrax.ControlTerm(self.func, control).to_ode()\n",
    "        solver = diffrax.Tsit5()\n",
    "        dt0 = None\n",
    "        y0 = self.initial(control.evaluate(ts[0]))\n",
    "        saveat = diffrax.SaveAt(ts=ts_eval)\n",
    "        \n",
    "        solution = diffrax.diffeqsolve(\n",
    "            term,\n",
    "            solver,\n",
    "            ts[0],\n",
    "            ts[-1],\n",
    "            dt0,\n",
    "            y0,\n",
    "            stepsize_controller=diffrax.PIDController(rtol=1e-3, atol=1e-6, jump_ts=ts),\n",
    "            saveat=saveat,\n",
    "        )\n",
    "        \n",
    "        prediction = jax.vmap(lambda y: jnn.relu(self.linear(y))[0])(solution.ys)\n",
    "\n",
    "        return prediction\n",
    "    \n",
    "    def compute_loss_batch(self, ts_batch, x_batch, ts_eval_batch, y_true_batch):\n",
    "        y_pred = self.predict_batch(ts_batch, x_batch, ts_eval_batch)\n",
    "        return jnp.mean((y_pred - y_true_batch) ** 2)\n",
    "    \n",
    "    def compute_loss(self, ts, x, ts_eval, y_true):\n",
    "        y_pred = self.predict(ts, x, ts_eval)\n",
    "        print(y_pred)\n",
    "        return jnp.mean((y_pred - y_true) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = 6\n",
    "hidden_size = 1\n",
    "width_size = 4\n",
    "depth = 2\n",
    "\n",
    "model = NeuralCDE(data_size, hidden_size, width_size, depth, key=key())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(77644.06, dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t, x, t_eval, y_eval = data[0]\n",
    "# t_batch, x_batch, t_eval_batch, y_eval_batch = data[0:8, 0], data[0:8, 1], data[0:8, 2], data[0:8, 3]\n",
    "\n",
    "# print(type(t_batch[0]))\n",
    "# print(type(x_batch[0]))\n",
    "# print(type(t_eval_batch[0]))\n",
    "# print(type(y_eval_batch[0]))\n",
    "\n",
    "\n",
    "# print(data[0:8])\n",
    "# print(len(x))\n",
    "# print(data[5][0][-2:])\n",
    "data[5][0][-1] = 2585\n",
    "t_batch, x_batch, t_eval_batch, y_eval_batch = map(np.array, zip(*data[0:8]))\n",
    "# t_batch[5][0][-1] = 2585\n",
    "# print(len(t_batch[0]))\n",
    "\n",
    "# for i in range(10):\n",
    "#     t, x, t_eval, y_eval = data[i]\n",
    "#     # if i == 5:\n",
    "#     #     t[-1] = 2585\n",
    "#     try:\n",
    "#         model.compute_loss(t, x, t_eval, y_eval)\n",
    "#     except:\n",
    "#         print(\"her etresdt easgrs\")\n",
    "#         print(t)\n",
    "model.compute_loss_batch(t_batch, x_batch, t_eval_batch, y_eval_batch)\n",
    "# model.compute_loss(data[0:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from equinox import EquinoxRuntimeError\n",
    "\n",
    "def train(model, train_data, optimizer, steps, batch_size, seq_length, eval_point_ratio, key, subset_size, patience=500, plot_every=500):\n",
    "    opt_state = optimizer.init(eqx.filter(model, eqx.is_inexact_array))\n",
    "    losses = []\n",
    "    best_loss = float('inf')\n",
    "    best_model = model\n",
    "    last_best_step = 0\n",
    "    step_times = []\n",
    "\n",
    "    @eqx.filter_value_and_grad\n",
    "    def loss(model, ts, xs, ts_test, y_true):\n",
    "        return model.compute_loss_batch(ts, xs, ts_test, y_true) \n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def make_step(model, opt_state, ts, xs, ts_test, y_true):\n",
    "        value, grads = loss(model, ts, xs, ts_test, y_true)\n",
    "        updates, opt_state = optimizer.update(grads, opt_state, model)\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        return value, model, opt_state\n",
    "    \n",
    "    num_params = sum(p.size for p in jax.tree_util.tree_leaves(eqx.filter(model, eqx.is_inexact_array)))\n",
    "    num_train_points = len(train_data) * seq_length * eval_point_ratio\n",
    "\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Number of parameters: {num_params}\")\n",
    "    print(f\"Number of data points: {int(num_train_points)}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    ts, xs, ts_eval, y_test = zip(*train_data)\n",
    "\n",
    "    arrays = (jnp.array(ts), jnp.array(xs), jnp.array(ts_eval), jnp.array(y_test))\n",
    "    data_gen = dataloader(arrays, batch_size, subset_size=subset_size, key=key)\n",
    "    # batch_num = 1\n",
    "\n",
    "    for step in range(steps):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Get the next batch from the dataloader\n",
    "        ts_batch, xs_batch, ts_test_batch, y_true_batch = next(data_gen)\n",
    "        ts_batch, xs_batch, ts_test_batch, y_true_batch = jnp.array(ts_batch), jnp.array(xs_batch), jnp.array(ts_test_batch), jnp.array(y_true_batch)\n",
    "        # ts_batch, xs_batch, ts_test_batch, y_true_batch = map(np.array, zip(*data[batch_num*batch_size: batch_num*(batch_size + 1)]))\n",
    "\n",
    "        # Perform a training step\n",
    "        try: \n",
    "            loss_value, model, opt_state = make_step(model, opt_state, ts_batch, xs_batch, ts_test_batch, y_true_batch)\n",
    "            losses.append(loss_value)\n",
    "        \n",
    "\n",
    "\n",
    "            step_time = time.time() - start_time\n",
    "            step_times.append(step_time)\n",
    "            if len(step_times) > 100:\n",
    "                step_times.pop(0)\n",
    "\n",
    "            if loss_value < best_loss:\n",
    "                best_loss = loss_value\n",
    "                best_model = model\n",
    "                last_best_step = step\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                avg_step_time = sum(step_times) / len(step_times) if step_times else 0\n",
    "                estimated_time_remaining = avg_step_time * (steps - step - 1)\n",
    "                if step == 0:\n",
    "                    print(f\"Step {step}, Loss: {loss_value:.4f}, Best Loss: {best_loss:.4f}, Estimated Time Remaining: -- \")\n",
    "                else:\n",
    "                    print(f\"Step {step}, Loss: {loss_value:.4f}, Best Loss: {best_loss:.4f}, Estimated Time Remaining: {estimated_time_remaining / 60:.2f} minutes\")\n",
    "\n",
    "            if step % plot_every == 0:\n",
    "                _, subkey = jr.split(key)\n",
    "                random_index = jr.randint(subkey, (1,), 0, batch_size).item()\n",
    "                plt.figure(figsize=(4, 3))\n",
    "                plt.plot(ts_test_batch[random_index], y_true_batch[random_index], label='Actual', marker='o')\n",
    "                y_pred = best_model.predict(jnp.expand_dims(ts_batch[random_index], axis=0), jnp.expand_dims(xs_batch[random_index], axis=0), jnp.expand_dims(ts_test_batch[random_index], axis=0))\n",
    "                y_pred = y_pred.squeeze()\n",
    "                plt.plot(ts_test_batch[random_index], y_pred, label='Predicted', marker='x')\n",
    "                plt.xlabel('Time (ts_test)')\n",
    "                plt.ylabel('Y values')\n",
    "                plt.title(f'Training Sequence at Step {step}: Actual vs Predicted Y over Time')\n",
    "                plt.legend()\n",
    "                plt.grid(True)\n",
    "                plt.show()\n",
    "        except EquinoxRuntimeError:\n",
    "            print(\"Error\")\n",
    "\n",
    "        if step - last_best_step >= patience:\n",
    "            print(f\"Stopping early at step {step}, no improvement for {patience} steps.\")\n",
    "            return best_model\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(losses, label=\"Training Loss\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Loss Over Time\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Number of parameters: 122\n",
      "Number of data points: 9830\n",
      "------------------------------------------------------------\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Stopping early at step 1178, no improvement for 500 steps.\n"
     ]
    }
   ],
   "source": [
    "data_size = 6\n",
    "hidden_size = 1\n",
    "width_size = 4\n",
    "depth = 2\n",
    "\n",
    "model = NeuralCDE(data_size, hidden_size, width_size, depth, key=key())\n",
    "\n",
    "eta = 0.01\n",
    "optimizer = optax.adam(learning_rate=eta)\n",
    "\n",
    "steps = 2000\n",
    "batch_size = 8\n",
    "seq_length = 32\n",
    "eval_point_ratio = 0.3\n",
    "subset_size = 128\n",
    "\n",
    "trained_model = train(\n",
    "    model,\n",
    "    data,\n",
    "    optimizer,\n",
    "    steps,\n",
    "    batch_size,\n",
    "    seq_length, \n",
    "    eval_point_ratio, \n",
    "    key, \n",
    "    subset_size,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
