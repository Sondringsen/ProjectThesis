{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import diffrax\n",
    "import equinox as eqx\n",
    "import jax\n",
    "import jax.nn as jnn\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import jax.lax as lax\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optax\n",
    "from functools import partial\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.path.join(os.getcwd(), \"../\"))\n",
    "from src.data.data_reader import DataReader\n",
    "\n",
    "key_number = 8\n",
    "\n",
    "def key():\n",
    "    global key_number\n",
    "    key_number += 1\n",
    "    return jax.random.PRNGKey(key_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# makes dummy data\n",
    "\n",
    "LATENT_NOISE_STD = 0.5  \n",
    "XS_FUNCTION_SCALE = 0.5 \n",
    "YS_FUNCTION_SCALE = 0.1\n",
    "\n",
    "\n",
    "def compute_xs_from_latent(latent, x_size):\n",
    "    key = jr.PRNGKey(0)\n",
    "    key, subkey = jr.split(key)\n",
    "    noise_xs = jr.normal(subkey, shape=(latent.shape[0], x_size)) * LATENT_NOISE_STD\n",
    "    \n",
    "    xs_list = []\n",
    "    for dim in range(x_size):\n",
    "        if dim % 3 == 0:\n",
    "            xs_component = XS_FUNCTION_SCALE * jnp.power(latent[:, dim % latent.shape[1]], 2) \n",
    "        elif dim % 3 == 1:\n",
    "            xs_component = XS_FUNCTION_SCALE * jnp.exp(-latent[:, dim % latent.shape[1]]) \n",
    "        else:\n",
    "            xs_component = XS_FUNCTION_SCALE * jnp.sin(latent[:, dim % latent.shape[1]])  \n",
    "        xs_list.append(xs_component)\n",
    "    xs = jnp.stack(xs_list, axis=-1) + noise_xs\n",
    "    return xs\n",
    "\n",
    "\n",
    "def compute_ys_from_latent(latent):\n",
    "    ys = YS_FUNCTION_SCALE * (jnp.sum(jnp.power(latent, 3), axis=1) + jnp.sum(jnp.sin(latent), axis=1)) \n",
    "    return ys\n",
    "\n",
    "\n",
    "def ode_dynamics(t, y, args):\n",
    "    damping = 0.1  \n",
    "    k = 1\n",
    "    equilibrium = jnp.array([1.0, 1.0])  \n",
    "\n",
    "    dydt = jnp.array([\n",
    "        -damping * y[0] + k * (equilibrium[0] - y[0]), \n",
    "        -damping * y[1] + k * (equilibrium[1] - y[1])   \n",
    "    ])\n",
    "    \n",
    "    return dydt\n",
    "\n",
    "def generate_latent_process(ts_combined, initial_latent=None):\n",
    "    key = jr.PRNGKey(0)\n",
    "    key, subkey = jr.split(key)\n",
    "    if initial_latent is None:\n",
    "        initial_latent = 10*jr.normal(subkey, shape=(2,)) \n",
    "    latent = initial_latent\n",
    "    latent_values = []\n",
    "    for i in range(len(ts_combined)):\n",
    "        if i > 0:\n",
    "            sol = diffrax.diffeqsolve(\n",
    "                diffrax.ODETerm(ode_dynamics),\n",
    "                diffrax.Tsit5(),\n",
    "                t0=ts_combined[i-1],\n",
    "                t1=ts_combined[i],\n",
    "                dt0=0.01,\n",
    "                y0=latent\n",
    "            )\n",
    "            latent = sol.ys[-1]\n",
    "        latent_values.append(latent)\n",
    "    return jnp.array(latent_values)\n",
    "\n",
    "\n",
    "def get_data(num_sequences=100, sequence_length=50, test_ratio=0.2, key=None, x_size=2):\n",
    "    train_data = []\n",
    "    for _ in range(num_sequences):\n",
    "        key, subkey = jr.split(key)\n",
    "        \n",
    "        ts = jr.uniform(subkey, shape=(sequence_length,), minval=0, maxval=10)\n",
    "        key, subkey = jr.split(key)\n",
    "        ts_test = jr.uniform(subkey, shape=(int(sequence_length * test_ratio),), minval=2, maxval=10)  \n",
    "        ts = jnp.sort(ts)\n",
    "        ts_test = jnp.sort(ts_test)\n",
    "        \n",
    "        ts_combined = jnp.sort(jnp.concatenate([ts, ts_test]))\n",
    "        latent_values_combined = generate_latent_process(ts_combined)\n",
    "        ts_indices = jnp.searchsorted(ts_combined, ts)\n",
    "        xs = compute_xs_from_latent(latent_values_combined[ts_indices], x_size)\n",
    "        \n",
    "        test_indices = jnp.searchsorted(ts_combined, ts_test)\n",
    "        y_test = compute_ys_from_latent(latent_values_combined[test_indices])\n",
    "        \n",
    "        train_data.append((ts, xs, ts_test, y_test))\n",
    "    \n",
    "    return train_data\n",
    "\n",
    "\n",
    "def dataloader(arrays, batch_size, *, key):\n",
    "    dataset_size = arrays[0].shape[0]\n",
    "    assert all(array.shape[0] == dataset_size for array in arrays)\n",
    "    indices = jnp.arange(dataset_size)\n",
    "    while True:\n",
    "        perm = jr.permutation(key, indices)\n",
    "        (key,) = jr.split(key, 1)\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        while start < dataset_size:\n",
    "            batch_perm = perm[start:end]\n",
    "            yield tuple(array[batch_perm] for array in arrays)\n",
    "            start = end\n",
    "            end = start + batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = get_data(num_sequences=32, sequence_length=16, test_ratio=0.25, key=key(), x_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jaxlib.xla_extension.ArrayImpl"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data[0][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data2(processed_file):\n",
    "    with open(processed_file, 'rb') as f:\n",
    "        sequences = pickle.load(f)\n",
    "\n",
    "    sorted_train_data = []\n",
    "    for idx, (ts, xs, ts_eval, ys_eval) in enumerate(sequences):\n",
    "        t0 = ts[0]\n",
    "        t_ms = (ts - t0) / 1e6  # Convert nanoseconds to milliseconds\n",
    "        te_ms = (ts_eval - t0) / 1e6\n",
    "\n",
    "        if len(t_ms) != len(xs):\n",
    "            print(f\"Warning: Inconsistent lengths in sequence {idx}: len(t_ms)={len(t_ms)}, len(xs)={len(xs)}\")\n",
    "            continue  \n",
    "\n",
    "        if len(te_ms) != len(ys_eval):\n",
    "            print(f\"Warning: Inconsistent lengths in ts_eval and ys_eval in sequence {idx}\")\n",
    "            continue \n",
    "\n",
    "        # Keep data as arrays\n",
    "        sorted_train_data.append((t_ms, xs, te_ms, ys_eval))\n",
    "\n",
    "    if not sorted_train_data:\n",
    "        raise ValueError(\"No valid sequences found in the processed data.\")\n",
    "\n",
    "    return sorted_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data2 = get_data2(\n",
    "    processed_file = \"../data/processed/Visa_2024-09-06.pkl\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data2[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Func is for the differential equation \n",
    "\n",
    "class Func(eqx.Module):\n",
    "    mlp: eqx.nn.MLP  \n",
    "    \n",
    "    def __init__(self, mlp):\n",
    "        self.mlp = mlp\n",
    "\n",
    "    def __call__(self, t, y, args):\n",
    "        return self.mlp(y) \n",
    "    \n",
    "#Latent ODE model\n",
    "\n",
    "class NeuralODEModel(eqx.Module):\n",
    "    initial_latent: jnp.ndarray\n",
    "    func: Func\n",
    "    rnn_cell: eqx.nn.GRUCell\n",
    "    latent_to_y: eqx.nn.MLP\n",
    "    latent_size: int\n",
    "    x_proj: eqx.nn.Linear  \n",
    "\n",
    "    def __init__(self, func, rnn_cell, latent_to_y, latent_size, x_size):\n",
    "        self.func = func\n",
    "        self.rnn_cell = rnn_cell\n",
    "        self.latent_to_y = latent_to_y\n",
    "        self.x_proj = eqx.nn.Linear(x_size, latent_size, key=key())\n",
    "        self.initial_latent = jnp.zeros((1, latent_size))  \n",
    "        \n",
    "        self.latent_size = latent_size\n",
    "\n",
    "    def _propagate_ode(self, latent, t0, t1):\n",
    "        sol = diffrax.diffeqsolve(\n",
    "            diffrax.ODETerm(self.func),\n",
    "            diffrax.Tsit5(),\n",
    "            t0=t0,\n",
    "            t1=t1,\n",
    "            dt0=0.1,\n",
    "            y0=latent,\n",
    "        )\n",
    "        return sol.ys[-1]\n",
    "\n",
    "    def predict(self, ts, xs, ts_test):\n",
    "        latent = jnp.tile(self.initial_latent, (ts.shape[0], 1))  \n",
    "        ts_combined = jnp.sort(jnp.concatenate([ts, ts_test], axis=1))  \n",
    "        ys_pred = jnp.zeros((ts_test.shape[0], ts_test.shape[1], 1)) \n",
    "\n",
    "        test_index = 0\n",
    "\n",
    "        for i in range(1, ts_combined.shape[1]):\n",
    "            t0, t1 = ts_combined[:, i-1], ts_combined[:, i]\n",
    "            latent = jax.vmap(self._propagate_ode)(latent, t0, t1) \n",
    "\n",
    "            latent = lax.cond(\n",
    "                jnp.any(jnp.isin(t1, ts[:, :ts.shape[1]])),\n",
    "                lambda lt: jax.vmap(lambda lt, x_lat: self.rnn_cell(lt, x_lat))(lt, jax.vmap(self.x_proj)(xs[:, i-1, :])),\n",
    "                lambda lt: lt,\n",
    "                latent\n",
    "            )\n",
    "\n",
    "\n",
    "            def update_prediction(yp, latent):\n",
    "                y_pred = jax.vmap(self.latent_to_y)(latent)\n",
    "                return yp.at[:, test_index, 0].set(y_pred.squeeze())\n",
    "\n",
    "            ys_pred = lax.cond(\n",
    "                jnp.any(jnp.isin(t1, ts_test)),\n",
    "                lambda yp: update_prediction(yp, latent),\n",
    "                lambda yp: yp,\n",
    "                ys_pred\n",
    "            )\n",
    "            \n",
    "            test_index = lax.cond(\n",
    "                jnp.any(jnp.isin(t1, ts_test)),\n",
    "                lambda idx: idx + 1,\n",
    "                lambda idx: idx,\n",
    "                test_index\n",
    "            )\n",
    "\n",
    "        return ys_pred.squeeze()\n",
    "\n",
    "    def compute_loss(self, ts, xs, ts_test, y_true):\n",
    "        y_pred = self.predict(ts, xs, ts_test) \n",
    "        return self.loss(y_pred, y_true)\n",
    "\n",
    "    @staticmethod\n",
    "    def loss(y_pred, y_true):\n",
    "        return jnp.mean((y_pred - y_true) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, optimizer, steps, batch_size, seq_length, eval_point_ratio, patience=500, plot_every=500):\n",
    "    opt_state = optimizer.init(eqx.filter(model, eqx.is_inexact_array))\n",
    "    losses = []\n",
    "    best_loss = float('inf')\n",
    "    best_model = model\n",
    "    last_best_step = 0\n",
    "    step_times = []\n",
    "\n",
    "    @eqx.filter_value_and_grad\n",
    "    def loss(model, ts, xs, ts_test, y_true):\n",
    "        return model.compute_loss(ts, xs, ts_test, y_true)\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def make_step(model, opt_state, ts, xs, ts_test, y_true):\n",
    "        value, grads = loss(model, ts, xs, ts_test, y_true)\n",
    "        updates, opt_state = optimizer.update(grads, opt_state, model)\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        return value, model, opt_state\n",
    "    \n",
    "    num_params = sum(p.size for p in jax.tree_util.tree_leaves(eqx.filter(model, eqx.is_inexact_array)))\n",
    "    num_train_points = len(train_data) * seq_length * eval_point_ratio\n",
    "\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Number of parameters: {num_params}\")\n",
    "    print(f\"Number of data points: {int(num_train_points)}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for step in range(steps):\n",
    "        start_time = time.time()\n",
    "\n",
    "        for idx in range(0, len(train_data), batch_size):\n",
    "            batch = train_data[idx:idx + batch_size]\n",
    "            ts_batch = jnp.stack([data[0] for data in batch])  \n",
    "            xs_batch = jnp.stack([data[1] for data in batch])  \n",
    "            ts_test_batch = jnp.stack([data[2] for data in batch])  \n",
    "            y_true_batch = jnp.stack([data[3] for data in batch])  \n",
    "\n",
    "            loss_value, model, opt_state = make_step(model, opt_state, ts_batch, xs_batch, ts_test_batch, y_true_batch)\n",
    "        losses.append(loss_value)\n",
    "\n",
    "        step_time = time.time() - start_time\n",
    "        step_times.append(step_time)\n",
    "        if len(step_times) > 100:\n",
    "            step_times.pop(0)\n",
    "\n",
    "        if loss_value < best_loss:\n",
    "            best_loss = loss_value\n",
    "            best_model = model\n",
    "            last_best_step = step\n",
    "\n",
    "        if step % 100 == 0:  \n",
    "            avg_step_time = sum(step_times) / len(step_times) if step_times else 0\n",
    "            estimated_time_remaining = avg_step_time * (steps - step - 1)\n",
    "            if step == 0:\n",
    "                print(f\"Step {step}, Loss: {loss_value:.4f}, Best Loss: {best_loss:.4f}, Estimated Time Remaining: -- \")\n",
    "            else:\n",
    "                print(f\"Step {step}, Loss: {loss_value:.4f}, Best Loss: {best_loss:.4f}, Estimated Time Remaining: {estimated_time_remaining / 60:.2f} minutes\")\n",
    "\n",
    "        if step % plot_every == 0:\n",
    "            _, subkey = jr.split(key())\n",
    "            random_index = jr.randint(subkey, (1,), 0, batch_size).item()\n",
    "            plt.figure(figsize=(4, 3))\n",
    "            plt.plot(ts_test_batch[random_index], y_true_batch[random_index], label='Actual', marker='o')\n",
    "            y_pred = best_model.predict(jnp.expand_dims(ts_batch[random_index], axis=0), jnp.expand_dims(xs_batch[random_index], axis=0), jnp.expand_dims(ts_test_batch[random_index], axis=0))\n",
    "            y_pred = y_pred.squeeze()\n",
    "            plt.plot(ts_test_batch[random_index], y_pred, label='Predicted', marker='x')\n",
    "            plt.xlabel('Time (ts_test)')\n",
    "            plt.ylabel('Y values')\n",
    "            plt.title(f'Training Sequence at Step {step}: Actual vs Predicted Y over Time')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "\n",
    "        if step - last_best_step >= patience:\n",
    "            print(f\"Stopping early at step {step}, no improvement for {patience} steps.\")\n",
    "            return best_model\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(losses, label=\"Training Loss\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Loss Over Time\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Number of parameters: 127\n",
      "Number of data points: 8192\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "EquinoxRuntimeError",
     "evalue": "Above is the stack outside of JIT. Below is the stack inside of JIT:\n  File \"/Users/erikmjaanes/opt/anaconda3/lib/python3.9/site-packages/diffrax/_integrate.py\", line 1423, in diffeqsolve\n    sol = result.error_if(sol, jnp.invert(is_okay(result)))\nequinox.EquinoxRuntimeError: The maximum number of solver steps was reached. Try increasing `max_steps`.\n\n-------------------\n\nAn error occurred during the runtime of your JAX program.\n\n1) Setting the environment variable `EQX_ON_ERROR=breakpoint` is usually the most useful\nway to debug such errors. This can be interacted with using most of the usual commands\nfor the Python debugger: `u` and `d` to move up and down frames, the name of a variable\nto print its value, etc.\n\n2) You may also like to try setting `JAX_DISABLE_JIT=1`. This will mean that you can\n(mostly) inspect the state of your program as if it was normal Python.\n\n3) See `https://docs.kidger.site/equinox/api/debug/` for more suggestions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEquinoxRuntimeError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/js/9hpr3tt17xd6ttv9xy3zrhw80000gn/T/ipykernel_78629/3466259185.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_seq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_point_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_patience\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplot_every\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/js/9hpr3tt17xd6ttv9xy3zrhw80000gn/T/ipykernel_78629/3455881165.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_data, optimizer, steps, batch_size, seq_length, eval_point_ratio, patience, plot_every)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0my_true_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mts_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mts_test_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/equinox/_jit.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, is_lower, args, kwargs)\u001b[0m\n\u001b[1;32m    224\u001b[0m                     \u001b[0;31m# callback necessarily executed in the same interpreter as we are in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                     \u001b[0;31m# here?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m                     raise EquinoxRuntimeError(\n\u001b[0m\u001b[1;32m    227\u001b[0m                         \u001b[0m_on_error_msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlast_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlast_stack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m                     ) from None\n",
      "\u001b[0;31mEquinoxRuntimeError\u001b[0m: Above is the stack outside of JIT. Below is the stack inside of JIT:\n  File \"/Users/erikmjaanes/opt/anaconda3/lib/python3.9/site-packages/diffrax/_integrate.py\", line 1423, in diffeqsolve\n    sol = result.error_if(sol, jnp.invert(is_okay(result)))\nequinox.EquinoxRuntimeError: The maximum number of solver steps was reached. Try increasing `max_steps`.\n\n-------------------\n\nAn error occurred during the runtime of your JAX program.\n\n1) Setting the environment variable `EQX_ON_ERROR=breakpoint` is usually the most useful\nway to debug such errors. This can be interacted with using most of the usual commands\nfor the Python debugger: `u` and `d` to move up and down frames, the name of a variable\nto print its value, etc.\n\n2) You may also like to try setting `JAX_DISABLE_JIT=1`. This will mean that you can\n(mostly) inspect the state of your program as if it was normal Python.\n\n3) See `https://docs.kidger.site/equinox/api/debug/` for more suggestions.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "train_steps = 2000\n",
    "x_size = 6\n",
    "eta = 5e-3\n",
    "num_train_seq = 1024\n",
    "train_seq_len = 32\n",
    "train_test_ratio = 0.25 #note: This is the ratio of ys to (xs + ys) in the training set, not the train-test split ratio\n",
    "func_mlp_width = 4\n",
    "func_mlp_depth = 2\n",
    "latent_size = 2\n",
    "pred_mlp_width = 4\n",
    "pred_mlp_depth = 2\n",
    "num_test_seq = 5\n",
    "test_seq_len = 200\n",
    "test_test_ratio = 0.3\n",
    "train_patience = 200\n",
    "plot_every = 100\n",
    "\n",
    "mlp = eqx.nn.MLP(in_size=latent_size, out_size=latent_size, width_size=func_mlp_width, depth=func_mlp_depth, key=key())  \n",
    "func = Func(mlp=mlp) \n",
    "\n",
    "rnn_cell = eqx.nn.GRUCell(input_size=latent_size, hidden_size=latent_size, key=key())  \n",
    "latent_to_y = eqx.nn.MLP(in_size=latent_size, out_size=1, width_size=pred_mlp_width, depth=pred_mlp_depth, key=key())\n",
    "\n",
    "model = NeuralODEModel(func=func, rnn_cell=rnn_cell, latent_to_y=latent_to_y, latent_size=latent_size, x_size=x_size)\n",
    "\n",
    "optimizer = optax.adam(learning_rate=eta)\n",
    "\n",
    "trained_model = train(model, train_data2, optimizer, steps=train_steps, batch_size=batch_size, seq_length=train_seq_len, eval_point_ratio = train_test_ratio, patience = train_patience, plot_every=plot_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
