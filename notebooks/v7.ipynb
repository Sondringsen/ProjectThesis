{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import diffrax\n",
    "import equinox as eqx\n",
    "import jax\n",
    "import jax.nn as jnn\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import jax.lax as lax\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optax\n",
    "from functools import partial\n",
    "import pickle\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.path.join(os.getcwd(), \"../\"))\n",
    "from src.data.data_reader import DataReader\n",
    "\n",
    "key_number = 8\n",
    "\n",
    "def key():\n",
    "    global key_number\n",
    "    key_number += 1\n",
    "    return jax.random.PRNGKey(key_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(processed_file):\n",
    "    with open(processed_file, 'rb') as f:\n",
    "        sequences = pickle.load(f)\n",
    "\n",
    "    sorted_train_data = []\n",
    "    for idx, (ts, xs, ts_eval, ys_eval) in enumerate(sequences):\n",
    "        t0 = ts[0]\n",
    "        t_ms = (ts - t0) / 1e6 # Convert nanoseconds to milliseconds\n",
    "        te_ms = (ts_eval - t0) / 1e6\n",
    "\n",
    "        if len(t_ms) != len(xs):\n",
    "            print(f\"Warning: Inconsistent lengths in sequence {idx}: len(t_ms)={len(t_ms)}, len(xs)={len(xs)}\")\n",
    "            continue  \n",
    "\n",
    "        if len(te_ms) != len(ys_eval):\n",
    "            print(f\"Warning: Inconsistent lengths in ts_eval and ys_eval in sequence {idx}\")\n",
    "            continue \n",
    "        \n",
    "        sorted_indices = np.argsort(t_ms)\n",
    "        t_ms = t_ms[sorted_indices]\n",
    "        xs = xs[sorted_indices]\n",
    "\n",
    "        sorted_indices = np.argsort(te_ms)\n",
    "        te_ms = te_ms[sorted_indices]\n",
    "        ys_eval = ys_eval[sorted_indices]\n",
    "\n",
    "        # Keep data as arrays\n",
    "        sorted_train_data.append((t_ms, xs, te_ms, ys_eval))\n",
    "\n",
    "\n",
    "    if not sorted_train_data:\n",
    "        raise ValueError(\"No valid sequences found in the processed data.\")\n",
    "\n",
    "    return sorted_train_data\n",
    "\n",
    "def dataloader(sequences, batch_size, subset_size, *, key):\n",
    "    dataset_size = len(sequences[0])\n",
    "    assert all(len(seq) == dataset_size for seq in sequences)\n",
    "    indices = np.arange(dataset_size)\n",
    "\n",
    "    while True:\n",
    "        subset_perm = np.random.choice(indices, size=subset_size, replace=False)\n",
    "\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "\n",
    "        while start < subset_size:\n",
    "            batch_perm = subset_perm[start:end]\n",
    "            # Ensure data remains as arrays\n",
    "            batch = [ [seq[i] for i in batch_perm] for seq in sequences ]\n",
    "            yield batch\n",
    "            start = end\n",
    "            end = start + batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data(\"../data/processed/Visa_2024-09-06.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n",
      "4\n",
      "24\n",
      "24\n",
      "8\n",
      "8\n",
      "4\n",
      "24\n",
      "24\n",
      "8\n",
      "8\n",
      "6\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(len(data))\n",
    "print(len(data[0]))\n",
    "print(len(data[0][0]))\n",
    "print(len(data[0][1]))\n",
    "print(len(data[0][2]))\n",
    "print(len(data[0][3]))\n",
    "\n",
    "print(len(data[1]))\n",
    "print(len(data[1][0]))\n",
    "print(len(data[1][1]))\n",
    "print(len(data[1][2]))\n",
    "print(len(data[1][3]))\n",
    "\n",
    "print(len(data[0][1][0]))\n",
    "print(type(data[0][1][0]))\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Func(eqx.Module):\n",
    "    mlp: eqx.nn.MLP\n",
    "    data_size: int\n",
    "    hidden_size: int\n",
    "\n",
    "    def __init__(self, data_size, hidden_size, width_size, depth, *, key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.data_size = data_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.mlp = eqx.nn.MLP(\n",
    "            in_size=hidden_size,\n",
    "            out_size=hidden_size * data_size,\n",
    "            width_size=width_size,\n",
    "            depth=depth,\n",
    "            activation=jnn.softplus,\n",
    "            # Note the use of a tanh final activation function. This is important to\n",
    "            # stop the model blowing up. (Just like how GRUs and LSTMs constrain the\n",
    "            # rate of change of their hidden states.)\n",
    "            final_activation=jnn.tanh,\n",
    "            key=key,\n",
    "        )\n",
    "\n",
    "    def __call__(self, t, y, args):\n",
    "        return self.mlp(y).reshape(self.hidden_size, self.data_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralCDE(eqx.Module):\n",
    "    initial: eqx.nn.MLP\n",
    "    func: Func\n",
    "    linear: eqx.nn.Linear\n",
    "\n",
    "    def __init__(self, data_size, hidden_size, width_size, depth, *, key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        ikey, fkey, lkey = jr.split(key, 3)\n",
    "        self.initial = eqx.nn.MLP(data_size + 1, hidden_size, width_size, depth, key=ikey)\n",
    "        self.func = Func(data_size + 1, hidden_size, width_size, depth, key=fkey)\n",
    "        self.linear = eqx.nn.Linear(hidden_size, 1, key=lkey)\n",
    "\n",
    "    def predict_batch(self, ts_batch, x_batch, ts_eval_batch):\n",
    "        # predictions = []\n",
    "        predictions = jax.vmap(self.predict, in_axes=(0, 0, None))(ts_batch, x_batch, ts_eval_batch)\n",
    "        # for ts, x, ts_eval in zip(ts_batch, x_batch, ts_eval_batch):\n",
    "        #     predictions.append(self.predict(ts, x, ts_eval))\n",
    "\n",
    "        return jnp.array(predictions)\n",
    "\n",
    "    def predict(self, ts, x, ts_eval):\n",
    "        ts, x = diffrax.rectilinear_interpolation(ts, x)\n",
    "        data = np.hstack([ts.reshape(-1, 1), x])\n",
    "        data = jnp.array(data)\n",
    "\n",
    "        control = diffrax.LinearInterpolation(ts, data)\n",
    "\n",
    "        term = diffrax.ControlTerm(self.func, control).to_ode()\n",
    "        solver = diffrax.Tsit5()\n",
    "        dt0 = None\n",
    "        y0 = self.initial(control.evaluate(ts[0]))\n",
    "        saveat = diffrax.SaveAt(ts=ts_eval)\n",
    "        \n",
    "        solution = diffrax.diffeqsolve(\n",
    "            term,\n",
    "            solver,\n",
    "            ts[0],\n",
    "            ts[-1],\n",
    "            dt0,\n",
    "            y0,\n",
    "            stepsize_controller=diffrax.PIDController(rtol=1e-3, atol=1e-6, jump_ts=ts),\n",
    "            saveat=saveat,\n",
    "        )\n",
    "        \n",
    "        prediction = jax.vmap(lambda y: jnn.relu(self.linear(y))[0])(solution.ys)\n",
    "\n",
    "        return prediction\n",
    "    \n",
    "    def compute_loss_batch(self, ts_batch, x_batch, ts_eval_batch, y_true_batch):\n",
    "        y_pred = self.predict_batch(ts_batch, x_batch, ts_eval_batch)\n",
    "        print(y_pred)\n",
    "        return jnp.mean((y_pred - y_true_batch) ** 2)\n",
    "    \n",
    "    def compute_loss(self, ts, x, ts_eval, y_true):\n",
    "        y_pred = self.predict(ts, x, ts_eval)\n",
    "        print(y_pred)\n",
    "        return jnp.mean((y_pred - y_true) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = 6\n",
    "hidden_size = 1\n",
    "width_size = 4\n",
    "depth = 2\n",
    "\n",
    "model = NeuralCDE(data_size, hidden_size, width_size, depth, key=key())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    },
    {
     "ename": "TracerArrayConversionError",
     "evalue": "The numpy.ndarray conversion method __array__() was called on traced array with shape float32[47,1]\nThis BatchTracer with object id 11006706992 was created on line:\n  /var/folders/3t/5vvh8l5x48s7tyvbx4b6v9xr0000gn/T/ipykernel_90187/2632247043.py:23:26 (NeuralCDE.predict)\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerArrayConversionError",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTracerArrayConversionError\u001b[0m                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[165], line 26\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(t_batch[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# for i in range(10):\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#     t, x, t_eval, y_eval = data[i]\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#     # if i == 5:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#         print(\"her etresdt easgrs\")\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#         print(t)\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_eval_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_eval_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# model.compute_loss(data[0:8])\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[163], line 50\u001b[0m, in \u001b[0;36mNeuralCDE.compute_loss_batch\u001b[0;34m(self, ts_batch, x_batch, ts_eval_batch, y_true_batch)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_loss_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m, ts_batch, x_batch, ts_eval_batch, y_true_batch):\n\u001b[0;32m---> 50\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mts_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mts_eval_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mprint\u001b[39m(y_pred)\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39mmean((y_pred \u001b[38;5;241m-\u001b[39m y_true_batch) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[163], line 15\u001b[0m, in \u001b[0;36mNeuralCDE.predict_batch\u001b[0;34m(self, ts_batch, x_batch, ts_eval_batch)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m, ts_batch, x_batch, ts_eval_batch):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# predictions = []\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mts_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mts_eval_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# for ts, x, ts_eval in zip(ts_batch, x_batch, ts_eval_batch):\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m#     predictions.append(self.predict(ts, x, ts_eval))\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39marray(predictions)\n",
      "    \u001b[0;31m[... skipping hidden 4 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[163], line 23\u001b[0m, in \u001b[0;36mNeuralCDE.predict\u001b[0;34m(self, ts, x, ts_eval)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, ts, x, ts_eval):\n\u001b[1;32m     22\u001b[0m     ts, x \u001b[38;5;241m=\u001b[39m diffrax\u001b[38;5;241m.\u001b[39mrectilinear_interpolation(ts, x)\n\u001b[0;32m---> 23\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     data \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39marray(data)\n\u001b[1;32m     26\u001b[0m     control \u001b[38;5;241m=\u001b[39m diffrax\u001b[38;5;241m.\u001b[39mLinearInterpolation(ts, data)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/core/shape_base.py:352\u001b[0m, in \u001b[0;36mhstack\u001b[0;34m(tup, dtype, casting)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_vhstack_dispatcher)\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhstack\u001b[39m(tup, \u001b[38;5;241m*\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, casting\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msame_kind\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    294\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;124;03m    Stack arrays in sequence horizontally (column wise).\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m \n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 352\u001b[0m     arrs \u001b[38;5;241m=\u001b[39m \u001b[43matleast_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arrs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    354\u001b[0m         arrs \u001b[38;5;241m=\u001b[39m [arrs]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/core/shape_base.py:65\u001b[0m, in \u001b[0;36matleast_1d\u001b[0;34m(*arys)\u001b[0m\n\u001b[1;32m     63\u001b[0m res \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ary \u001b[38;5;129;01min\u001b[39;00m arys:\n\u001b[0;32m---> 65\u001b[0m     ary \u001b[38;5;241m=\u001b[39m \u001b[43masanyarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ary\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     67\u001b[0m         result \u001b[38;5;241m=\u001b[39m ary\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/core.py:714\u001b[0m, in \u001b[0;36mTracer.__array__\u001b[0;34m(self, *args, **kw)\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m--> 714\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m TracerArrayConversionError(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mTracerArrayConversionError\u001b[0m: The numpy.ndarray conversion method __array__() was called on traced array with shape float32[47,1]\nThis BatchTracer with object id 11006706992 was created on line:\n  /var/folders/3t/5vvh8l5x48s7tyvbx4b6v9xr0000gn/T/ipykernel_90187/2632247043.py:23:26 (NeuralCDE.predict)\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerArrayConversionError"
     ]
    }
   ],
   "source": [
    "t, x, t_eval, y_eval = data[0]\n",
    "# t_batch, x_batch, t_eval_batch, y_eval_batch = data[0:8, 0], data[0:8, 1], data[0:8, 2], data[0:8, 3]\n",
    "\n",
    "# print(type(t_batch[0]))\n",
    "# print(type(x_batch[0]))\n",
    "# print(type(t_eval_batch[0]))\n",
    "# print(type(y_eval_batch[0]))\n",
    "\n",
    "\n",
    "# print(data[0:8])\n",
    "# print(len(x))\n",
    "data[5][0][-1]= 2585\n",
    "t_batch, x_batch, t_eval_batch, y_eval_batch = map(np.array, zip(*data[0:8]))\n",
    "# t_batch[5][0][-1] = 2585\n",
    "print(len(t_batch[0]))\n",
    "\n",
    "# for i in range(10):\n",
    "#     t, x, t_eval, y_eval = data[i]\n",
    "#     # if i == 5:\n",
    "#     #     t[-1] = 2585\n",
    "#     try:\n",
    "#         model.compute_loss(t, x, t_eval, y_eval)\n",
    "#     except:\n",
    "#         print(\"her etresdt easgrs\")\n",
    "#         print(t)\n",
    "model.compute_loss_batch(t_batch, x_batch, t_eval_batch, y_eval_batch)\n",
    "# model.compute_loss(data[0:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
