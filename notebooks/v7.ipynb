{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import diffrax\n",
    "import equinox as eqx\n",
    "import jax\n",
    "import jax.nn as jnn\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import jax.lax as lax\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optax\n",
    "from functools import partial\n",
    "import pickle\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.path.join(os.getcwd(), \"../\"))\n",
    "from src.data.data_reader import DataReader\n",
    "\n",
    "key_number = 8\n",
    "\n",
    "def key():\n",
    "    global key_number\n",
    "    key_number += 1\n",
    "    return jax.random.PRNGKey(key_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(processed_file):\n",
    "    with open(processed_file, 'rb') as f:\n",
    "        sequences = pickle.load(f)\n",
    "    \n",
    "    sorted_train_data = []\n",
    "    for idx, (ts, xs, ts_eval, ys_eval) in enumerate(sequences):\n",
    "        t0 = ts[0]\n",
    "        t_ms = (ts - t0) / 1e6 # Convert nanoseconds to milliseconds\n",
    "        te_ms = (ts_eval - t0) / 1e6\n",
    "\n",
    "        if len(t_ms) != len(xs):\n",
    "            print(f\"Warning: Inconsistent lengths in sequence {idx}: len(t_ms)={len(t_ms)}, len(xs)={len(xs)}\")\n",
    "            continue  \n",
    "\n",
    "        if len(te_ms) != len(ys_eval):\n",
    "            print(f\"Warning: Inconsistent lengths in ts_eval and ys_eval in sequence {idx}\")\n",
    "            continue \n",
    "        \n",
    "        sorted_indices = np.argsort(t_ms)\n",
    "        t_ms = t_ms[sorted_indices]\n",
    "        xs = xs[sorted_indices]\n",
    "\n",
    "        sorted_indices = np.argsort(te_ms)\n",
    "        te_ms = te_ms[sorted_indices]\n",
    "        ys_eval = ys_eval[sorted_indices]\n",
    "\n",
    "        # Keep data as arrays\n",
    "        sorted_train_data.append((t_ms, xs, te_ms, ys_eval))\n",
    "\n",
    "\n",
    "    if not sorted_train_data:\n",
    "        raise ValueError(\"No valid sequences found in the processed data.\")\n",
    "\n",
    "    return sorted_train_data\n",
    "\n",
    "def dataloader(sequences, batch_size, subset_size, *, key):\n",
    "    dataset_size = len(sequences[0])\n",
    "    assert all(len(seq) == dataset_size for seq in sequences)\n",
    "    indices = np.arange(dataset_size)\n",
    "\n",
    "    while True:\n",
    "        subset_perm = np.random.choice(indices, size=subset_size, replace=False)\n",
    "\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "\n",
    "        while start < subset_size:\n",
    "            batch_perm = subset_perm[start:end]\n",
    "            # Ensure data remains as arrays\n",
    "            batch = [ [seq[i] for i in batch_perm] for seq in sequences ]\n",
    "            yield batch\n",
    "            start = end\n",
    "            end = start + batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data(\"../data/processed/Visa_2024-09-06.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n",
      "4\n",
      "24\n",
      "24\n",
      "8\n",
      "8\n",
      "4\n",
      "24\n",
      "24\n",
      "8\n",
      "8\n",
      "6\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(len(data))\n",
    "print(len(data[0]))\n",
    "print(len(data[0][0]))\n",
    "print(len(data[0][1]))\n",
    "print(len(data[0][2]))\n",
    "print(len(data[0][3]))\n",
    "\n",
    "print(len(data[1]))\n",
    "print(len(data[1][0]))\n",
    "print(len(data[1][1]))\n",
    "print(len(data[1][2]))\n",
    "print(len(data[1][3]))\n",
    "\n",
    "print(len(data[0][1][0]))\n",
    "print(type(data[0][1][0]))\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Func(eqx.Module):\n",
    "    mlp: eqx.nn.MLP\n",
    "    data_size: int\n",
    "    hidden_size: int\n",
    "\n",
    "    def __init__(self, data_size, hidden_size, width_size, depth, *, key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.data_size = data_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.mlp = eqx.nn.MLP(\n",
    "            in_size=hidden_size,\n",
    "            out_size=hidden_size * data_size,\n",
    "            width_size=width_size,\n",
    "            depth=depth,\n",
    "            activation=jnn.softplus,\n",
    "            # Note the use of a tanh final activation function. This is important to\n",
    "            # stop the model blowing up. (Just like how GRUs and LSTMs constrain the\n",
    "            # rate of change of their hidden states.)\n",
    "            final_activation=jnn.tanh,\n",
    "            key=key,\n",
    "        )\n",
    "\n",
    "    def __call__(self, t, y, args):\n",
    "        return self.mlp(y).reshape(self.hidden_size, self.data_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralCDE(eqx.Module):\n",
    "    initial: eqx.nn.MLP\n",
    "    func: Func\n",
    "    linear: eqx.nn.Linear\n",
    "\n",
    "    def __init__(self, data_size, hidden_size, width_size, depth, *, key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        ikey, fkey, lkey = jr.split(key, 3)\n",
    "        self.initial = eqx.nn.MLP(data_size + 1, hidden_size, width_size, depth, key=ikey)\n",
    "        self.func = Func(data_size + 1, hidden_size, width_size, depth, key=fkey)\n",
    "        self.linear = eqx.nn.Linear(hidden_size, 1, key=lkey)\n",
    "\n",
    "    def predict_batch(self, ts_batch, x_batch, ts_eval_batch):\n",
    "        # ts_batch = [ts.reshape(-1, 1) for ts in ts_batch]\n",
    "        predictions = jax.vmap(self.predict, in_axes=(0, 0, 0))(ts_batch, x_batch, ts_eval_batch)\n",
    "        # predictions = []\n",
    "        # for ts, x, ts_eval in zip(ts_batch, x_batch, ts_eval_batch):\n",
    "        #     predictions.append(self.predict(ts, x, ts_eval))\n",
    "\n",
    "        return jnp.array(predictions)\n",
    "\n",
    "    def predict(self, ts, x, ts_eval):\n",
    "        ts, x = diffrax.rectilinear_interpolation(ts, x)\n",
    "        data = jnp.hstack([ts.reshape(-1, 1), x])\n",
    "        data = jnp.array(data)\n",
    "\n",
    "        control = diffrax.LinearInterpolation(ts, data)\n",
    "\n",
    "        term = diffrax.ControlTerm(self.func, control).to_ode()\n",
    "        solver = diffrax.Tsit5()\n",
    "        dt0 = None\n",
    "        y0 = self.initial(control.evaluate(ts[0]))\n",
    "        saveat = diffrax.SaveAt(ts=ts_eval)\n",
    "        \n",
    "        solution = diffrax.diffeqsolve(\n",
    "            term,\n",
    "            solver,\n",
    "            ts[0],\n",
    "            ts[-1],\n",
    "            dt0,\n",
    "            y0,\n",
    "            stepsize_controller=diffrax.PIDController(rtol=1e-3, atol=1e-6, jump_ts=ts),\n",
    "            saveat=saveat,\n",
    "        )\n",
    "        \n",
    "        prediction = jax.vmap(lambda y: jnn.relu(self.linear(y))[0])(solution.ys)\n",
    "\n",
    "        return prediction\n",
    "    \n",
    "    def compute_loss_batch(self, ts_batch, x_batch, ts_eval_batch, y_true_batch):\n",
    "        y_pred = self.predict_batch(ts_batch, x_batch, ts_eval_batch)\n",
    "        return jnp.mean((y_pred - y_true_batch) ** 2)\n",
    "    \n",
    "    def compute_loss(self, ts, x, ts_eval, y_true):\n",
    "        y_pred = self.predict(ts, x, ts_eval)\n",
    "        print(y_pred)\n",
    "        return jnp.mean((y_pred - y_true) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = 6\n",
    "hidden_size = 1\n",
    "width_size = 4\n",
    "depth = 2\n",
    "\n",
    "model = NeuralCDE(data_size, hidden_size, width_size, depth, key=key())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(77644.06, dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t, x, t_eval, y_eval = data[0]\n",
    "# t_batch, x_batch, t_eval_batch, y_eval_batch = data[0:8, 0], data[0:8, 1], data[0:8, 2], data[0:8, 3]\n",
    "\n",
    "# print(type(t_batch[0]))\n",
    "# print(type(x_batch[0]))\n",
    "# print(type(t_eval_batch[0]))\n",
    "# print(type(y_eval_batch[0]))\n",
    "\n",
    "\n",
    "# print(data[0:8])\n",
    "# print(len(x))\n",
    "# print(data[5][0][-2:])\n",
    "data[5][0][-1] = 2585\n",
    "t_batch, x_batch, t_eval_batch, y_eval_batch = map(np.array, zip(*data[0:8]))\n",
    "# t_batch[5][0][-1] = 2585\n",
    "# print(len(t_batch[0]))\n",
    "\n",
    "# for i in range(10):\n",
    "#     t, x, t_eval, y_eval = data[i]\n",
    "#     # if i == 5:\n",
    "#     #     t[-1] = 2585\n",
    "#     try:\n",
    "#         model.compute_loss(t, x, t_eval, y_eval)\n",
    "#     except:\n",
    "#         print(\"her etresdt easgrs\")\n",
    "#         print(t)\n",
    "model.compute_loss_batch(t_batch, x_batch, t_eval_batch, y_eval_batch)\n",
    "# model.compute_loss(data[0:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, optimizer, steps, batch_size, seq_length, eval_point_ratio, key, subset_size, patience=500, plot_every=500):\n",
    "    opt_state = optimizer.init(eqx.filter(model, eqx.is_inexact_array))\n",
    "    losses = []\n",
    "    best_loss = float('inf')\n",
    "    best_model = model\n",
    "    last_best_step = 0\n",
    "    step_times = []\n",
    "\n",
    "    @eqx.filter_value_and_grad\n",
    "    def loss(model, ts, xs, ts_test, y_true):\n",
    "        return model.compute_loss_batch(ts, xs, ts_test, y_true) \n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def make_step(model, opt_state, ts, xs, ts_test, y_true):\n",
    "        value, grads = loss(model, ts, xs, ts_test, y_true)\n",
    "        updates, opt_state = optimizer.update(grads, opt_state, model)\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        return value, model, opt_state\n",
    "    \n",
    "    num_params = sum(p.size for p in jax.tree_util.tree_leaves(eqx.filter(model, eqx.is_inexact_array)))\n",
    "    num_train_points = len(train_data) * seq_length * eval_point_ratio\n",
    "\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Number of parameters: {num_params}\")\n",
    "    print(f\"Number of data points: {int(num_train_points)}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    ts, xs, ts_eval, y_test = zip(*train_data)\n",
    "\n",
    "    arrays = (jnp.array(ts), jnp.array(xs), jnp.array(ts_eval), jnp.array(y_test))\n",
    "    data_gen = dataloader(arrays, batch_size, subset_size=subset_size, key=key)\n",
    "    # batch_num = 1\n",
    "\n",
    "    for step in range(steps):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Get the next batch from the dataloader\n",
    "        ts_batch, xs_batch, ts_test_batch, y_true_batch = next(data_gen)\n",
    "        ts_batch, xs_batch, ts_test_batch, y_true_batch = jnp.array(ts_batch), jnp.array(xs_batch), jnp.array(ts_test_batch), jnp.array(y_true_batch)\n",
    "        # ts_batch, xs_batch, ts_test_batch, y_true_batch = map(np.array, zip(*data[batch_num*batch_size: batch_num*(batch_size + 1)]))\n",
    "\n",
    "        # Perform a training step\n",
    "        loss_value, model, opt_state = make_step(model, opt_state, ts_batch, xs_batch, ts_test_batch, y_true_batch)\n",
    "        losses.append(loss_value)\n",
    "\n",
    "        step_time = time.time() - start_time\n",
    "        step_times.append(step_time)\n",
    "        if len(step_times) > 100:\n",
    "            step_times.pop(0)\n",
    "\n",
    "        if loss_value < best_loss:\n",
    "            best_loss = loss_value\n",
    "            best_model = model\n",
    "            last_best_step = step\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            avg_step_time = sum(step_times) / len(step_times) if step_times else 0\n",
    "            estimated_time_remaining = avg_step_time * (steps - step - 1)\n",
    "            if step == 0:\n",
    "                print(f\"Step {step}, Loss: {loss_value:.4f}, Best Loss: {best_loss:.4f}, Estimated Time Remaining: -- \")\n",
    "            else:\n",
    "                print(f\"Step {step}, Loss: {loss_value:.4f}, Best Loss: {best_loss:.4f}, Estimated Time Remaining: {estimated_time_remaining / 60:.2f} minutes\")\n",
    "\n",
    "        if step % plot_every == 0:\n",
    "            _, subkey = jr.split(key)\n",
    "            random_index = jr.randint(subkey, (1,), 0, batch_size).item()\n",
    "            plt.figure(figsize=(4, 3))\n",
    "            plt.plot(ts_test_batch[random_index], y_true_batch[random_index], label='Actual', marker='o')\n",
    "            y_pred = best_model.predict(jnp.expand_dims(ts_batch[random_index], axis=0), jnp.expand_dims(xs_batch[random_index], axis=0), jnp.expand_dims(ts_test_batch[random_index], axis=0))\n",
    "            y_pred = y_pred.squeeze()\n",
    "            plt.plot(ts_test_batch[random_index], y_pred, label='Predicted', marker='x')\n",
    "            plt.xlabel('Time (ts_test)')\n",
    "            plt.ylabel('Y values')\n",
    "            plt.title(f'Training Sequence at Step {step}: Actual vs Predicted Y over Time')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "\n",
    "        if step - last_best_step >= patience:\n",
    "            print(f\"Stopping early at step {step}, no improvement for {patience} steps.\")\n",
    "            return best_model\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(losses, label=\"Training Loss\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Loss Over Time\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Number of parameters: 122\n",
      "Number of data points: 9830\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "EquinoxRuntimeError",
     "evalue": "Above is the stack outside of JIT. Below is the stack inside of JIT:\n  File \"/var/folders/3t/5vvh8l5x48s7tyvbx4b6v9xr0000gn/T/ipykernel_4598/950420050.py\", line 15, in make_step\n    value, grads = loss(model, ts, xs, ts_test, y_true)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/folders/3t/5vvh8l5x48s7tyvbx4b6v9xr0000gn/T/ipykernel_4598/950420050.py\", line 11, in loss\n    return model.compute_loss_batch(ts, xs, ts_test, y_true)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/folders/3t/5vvh8l5x48s7tyvbx4b6v9xr0000gn/T/ipykernel_4598/1657949946.py\", line 51, in compute_loss_batch\n    y_pred = self.predict_batch(ts_batch, x_batch, ts_eval_batch)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/folders/3t/5vvh8l5x48s7tyvbx4b6v9xr0000gn/T/ipykernel_4598/1657949946.py\", line 15, in predict_batch\n    predictions = jax.vmap(self.predict, in_axes=(0, 0, 0))(ts_batch, x_batch, ts_eval_batch)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/folders/3t/5vvh8l5x48s7tyvbx4b6v9xr0000gn/T/ipykernel_4598/1657949946.py\", line 23, in predict\n    ts, x = diffrax.rectilinear_interpolation(ts, x)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/diffrax/_global_interpolation.py\", line 595, in rectilinear_interpolation\n    ts = _check_ts(ts)\n         ^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/diffrax/_global_interpolation.py\", line 407, in _check_ts\n    ts = eqxi.error_if(\n         ^^^^^^^^^^^^^^\nequinox.EquinoxRuntimeError: `ts` must be monotonically strictly increasing.\n\n-------------------\n\nAn error occurred during the runtime of your JAX program.\n\n1) Setting the environment variable `EQX_ON_ERROR=breakpoint` is usually the most useful\nway to debug such errors. This can be interacted with using most of the usual commands\nfor the Python debugger: `u` and `d` to move up and down frames, the name of a variable\nto print its value, etc.\n\n2) You may also like to try setting `JAX_DISABLE_JIT=1`. This will mean that you can\n(mostly) inspect the state of your program as if it was normal Python.\n\n3) See `https://docs.kidger.site/equinox/api/debug/` for more suggestions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEquinoxRuntimeError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m eval_point_ratio \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.3\u001b[39m\n\u001b[1;32m     15\u001b[0m subset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m\n\u001b[0;32m---> 17\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseq_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_point_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubset_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 43\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_data, optimizer, steps, batch_size, seq_length, eval_point_ratio, key, subset_size, patience, plot_every)\u001b[0m\n\u001b[1;32m     39\u001b[0m ts_batch, xs_batch, ts_test_batch, y_true_batch \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39marray(ts_batch), jnp\u001b[38;5;241m.\u001b[39marray(xs_batch), jnp\u001b[38;5;241m.\u001b[39marray(ts_test_batch), jnp\u001b[38;5;241m.\u001b[39marray(y_true_batch)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# ts_batch, xs_batch, ts_test_batch, y_true_batch = map(np.array, zip(*data[batch_num*batch_size: batch_num*(batch_size + 1)]))\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Perform a training step\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m loss_value, model, opt_state \u001b[38;5;241m=\u001b[39m \u001b[43mmake_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mts_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxs_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mts_test_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_true_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss_value)\n\u001b[1;32m     46\u001b[0m step_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "\u001b[0;31mEquinoxRuntimeError\u001b[0m: Above is the stack outside of JIT. Below is the stack inside of JIT:\n  File \"/var/folders/3t/5vvh8l5x48s7tyvbx4b6v9xr0000gn/T/ipykernel_4598/950420050.py\", line 15, in make_step\n    value, grads = loss(model, ts, xs, ts_test, y_true)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/folders/3t/5vvh8l5x48s7tyvbx4b6v9xr0000gn/T/ipykernel_4598/950420050.py\", line 11, in loss\n    return model.compute_loss_batch(ts, xs, ts_test, y_true)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/folders/3t/5vvh8l5x48s7tyvbx4b6v9xr0000gn/T/ipykernel_4598/1657949946.py\", line 51, in compute_loss_batch\n    y_pred = self.predict_batch(ts_batch, x_batch, ts_eval_batch)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/folders/3t/5vvh8l5x48s7tyvbx4b6v9xr0000gn/T/ipykernel_4598/1657949946.py\", line 15, in predict_batch\n    predictions = jax.vmap(self.predict, in_axes=(0, 0, 0))(ts_batch, x_batch, ts_eval_batch)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/folders/3t/5vvh8l5x48s7tyvbx4b6v9xr0000gn/T/ipykernel_4598/1657949946.py\", line 23, in predict\n    ts, x = diffrax.rectilinear_interpolation(ts, x)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/diffrax/_global_interpolation.py\", line 595, in rectilinear_interpolation\n    ts = _check_ts(ts)\n         ^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/diffrax/_global_interpolation.py\", line 407, in _check_ts\n    ts = eqxi.error_if(\n         ^^^^^^^^^^^^^^\nequinox.EquinoxRuntimeError: `ts` must be monotonically strictly increasing.\n\n-------------------\n\nAn error occurred during the runtime of your JAX program.\n\n1) Setting the environment variable `EQX_ON_ERROR=breakpoint` is usually the most useful\nway to debug such errors. This can be interacted with using most of the usual commands\nfor the Python debugger: `u` and `d` to move up and down frames, the name of a variable\nto print its value, etc.\n\n2) You may also like to try setting `JAX_DISABLE_JIT=1`. This will mean that you can\n(mostly) inspect the state of your program as if it was normal Python.\n\n3) See `https://docs.kidger.site/equinox/api/debug/` for more suggestions.\n"
     ]
    }
   ],
   "source": [
    "data_size = 6\n",
    "hidden_size = 1\n",
    "width_size = 4\n",
    "depth = 2\n",
    "\n",
    "model = NeuralCDE(data_size, hidden_size, width_size, depth, key=key())\n",
    "\n",
    "eta = 0.01\n",
    "optimizer = optax.adam(learning_rate=eta)\n",
    "\n",
    "steps = 2000\n",
    "batch_size = 8\n",
    "seq_length = 32\n",
    "eval_point_ratio = 0.3\n",
    "subset_size = 128\n",
    "\n",
    "trained_model = train(\n",
    "    model,\n",
    "    data,\n",
    "    optimizer,\n",
    "    steps,\n",
    "    batch_size,\n",
    "    seq_length, \n",
    "    eval_point_ratio, \n",
    "    key, \n",
    "    subset_size,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
