{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import diffrax\n",
    "import equinox as eqx\n",
    "import jax\n",
    "import jax.nn as jnn\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optax\n",
    "\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.path.join(os.getcwd(), \"../\"))\n",
    "from src.data.data_reader import DataReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import pandas as pd\n",
    "\n",
    "# Function to generate a latent process using a simple ODE\n",
    "def generate_latent_process(num_steps=1000, dt=0.01, key=None):\n",
    "    key, subkey = jr.split(key)\n",
    "    \n",
    "    # Initialize the latent state\n",
    "    latent_state = jnp.zeros((num_steps,))\n",
    "    for i in range(1, num_steps):\n",
    "        # Simple ODE: dx/dt = -x + noise\n",
    "        noise = jr.normal(subkey, (1,)) * 0.1  # Adjust noise scale as needed\n",
    "        latent_state = latent_state.at[i].set(latent_state[i-1] - latent_state[i-1] * dt + noise)\n",
    "        subkey, noise_key = jr.split(subkey)\n",
    "\n",
    "    return latent_state\n",
    "\n",
    "def get_data(num_samples=1000, test_ratio=0.2, key=None):\n",
    "    # Generate a latent process\n",
    "    latent_process = generate_latent_process(num_steps=num_samples, key=key)\n",
    "\n",
    "    # Randomly sample timestamps from the latent process\n",
    "    ts = jr.choice(key, jnp.arange(num_samples), shape=(num_samples,))\n",
    "    ts_test = jr.choice(key, jnp.arange(num_samples), shape=(num_samples // test_ratio,))\n",
    "\n",
    "    # Generate noisy observations (xs) from the latent process\n",
    "    xs = latent_process[ts] + jr.normal(key, shape=(len(ts),)) * 0.05  # Add noise\n",
    "\n",
    "    # Generate true outputs (ys) as a function of the latent state\n",
    "    # Here, we can define a simple function like y = latent_state^2 + noise\n",
    "    ys = latent_process[ts] ** 2 + jr.normal(key, shape=(len(ts),)) * 0.1\n",
    "    \n",
    "    # Get the y_true values at the sampled test times\n",
    "    y_true = latent_process[ts_test]\n",
    "\n",
    "    return jnp.expand_dims(ts, axis=0), jnp.expand_dims(xs, axis=0), jnp.expand_dims(ys, axis=0), jnp.expand_dims(ts_test, axis=0), jnp.expand_dims(y_true, axis=0)\n",
    "\n",
    "def dataloader(arrays, batch_size, *, key):\n",
    "    dataset_size = arrays[0].shape[1]  # Adjusted for shape (1, N)\n",
    "    assert all(array.shape[1] == dataset_size for array in arrays)\n",
    "    \n",
    "    indices = jnp.arange(dataset_size)\n",
    "    while True:\n",
    "        perm = jr.permutation(key, indices)\n",
    "        (key,) = jr.split(key, 1)\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        while start < dataset_size:\n",
    "            batch_perm = perm[start:end]\n",
    "            yield tuple(array[:, batch_perm] for array in arrays)  # Adjust for shape (1, N)\n",
    "            start = end\n",
    "            end = start + batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(arrays, batch_size, *, key):\n",
    "    dataset_size = arrays[0].shape[0]\n",
    "    assert all(array.shape[0] == dataset_size for array in arrays)\n",
    "    indices = jnp.arange(dataset_size)\n",
    "    while True:\n",
    "        perm = jr.permutation(key, indices)\n",
    "        (key,) = jr.split(key, 1)\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        while start < dataset_size:\n",
    "            batch_perm = perm[start:end]\n",
    "            yield tuple(array[batch_perm] for array in arrays)\n",
    "            start = end\n",
    "            end = start + batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralODEModel(eqx.Module):\n",
    "    func: eqx.Module\n",
    "    rnn_cell: eqx.nn.GRUCell\n",
    "    latent_to_y: eqx.nn.MLP\n",
    "    x_to_latent: eqx.nn.Linear\n",
    "    latent_size: int\n",
    "\n",
    "    def __init__(self, func, rnn_cell, latent_to_y, latent_size, x_size):\n",
    "        self.func = func\n",
    "        self.rnn_cell = rnn_cell\n",
    "        self.latent_to_y = latent_to_y\n",
    "        self.x_to_latent = eqx.nn.Linear(x_size, latent_size, key=jax.random.PRNGKey(6))\n",
    "        self.latent_size = latent_size\n",
    "\n",
    "    def _propagate_ode(self, latent, t0, t1):\n",
    "        sol = diffrax.diffeqsolve(\n",
    "            diffrax.ODETerm(self.func),\n",
    "            diffrax.Tsit5(),\n",
    "            t0=t0,\n",
    "            t1=t1,\n",
    "            dt0=0.1,\n",
    "            y0=latent,\n",
    "        )\n",
    "        return sol.ys[-1]\n",
    "\n",
    "    def predict(self, ts, xs, ts_test):\n",
    "        ts_combined = jnp.sort(jnp.concatenate([ts, ts_test]))\n",
    "        is_test_time = jnp.isin(ts_combined, ts_test)\n",
    "        latent = jnp.zeros((self.latent_size,))\n",
    "        ys_pred = []\n",
    "        last_t = ts_combined[0]\n",
    "\n",
    "        for i, t in enumerate(ts_combined):\n",
    "            if i > 0:\n",
    "                latent = self._propagate_ode(latent, last_t, t)\n",
    "            if t in ts:\n",
    "                idx = jnp.where(ts == t)[0][0]\n",
    "                x = xs[idx]\n",
    "                x_latent = self.x_to_latent(x)\n",
    "                latent = self.rnn_cell(latent, x_latent)\n",
    "            if is_test_time[i]:\n",
    "                y_pred = self.latent_to_y(latent)\n",
    "                ys_pred.append(y_pred)\n",
    "            last_t = t\n",
    "\n",
    "        return jnp.stack(ys_pred)\n",
    "\n",
    "    \n",
    "    def compute_loss(self, ts, xs, ts_test, y_true):\n",
    "        y_pred = self.predict(ts, xs, ts_test)\n",
    "        return jnp.mean((y_pred - y_true) ** 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import equinox as eqx\n",
    "import optax\n",
    "import pandas as pd\n",
    "import jax.random as jr\n",
    "\n",
    "# Assume NeuralODEModel, get_data, and dataloader are defined as discussed\n",
    "\n",
    "def get_data(train_split=0.8):\n",
    "    d_path = \"../data/processed/msft.csv\"\n",
    "    df = pd.read_csv(d_path)\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df = df.drop_duplicates(subset='timestamp', keep='last')\n",
    "\n",
    "    # Convert timestamp to seconds and create xs (features)\n",
    "    ts = pd.to_datetime(df[\"timestamp\"]).astype(int) / 1_000_000_000  # Convert to seconds\n",
    "    ts = jnp.array(ts)\n",
    "    xs = jnp.column_stack((df[\"bid_price\"].to_numpy(), df[\"offer_price\"].to_numpy()))  # Features\n",
    "    ys = df[\"next_price\"].to_numpy()  # Next price as the target variable\n",
    "\n",
    "    # Normalize ts, xs, and ys\n",
    "    ts = (ts - ts.min()) / (ts.max() - ts.min())  # Normalize ts between 0 and 1\n",
    "    xs = (xs - xs.min(axis=0)) / (xs.max(axis=0) - xs.min(axis=0))  # Normalize each feature in xs\n",
    "    ys = (ys - ys.min()) / (ys.max() - ys.min())  # Normalize ys between 0 and 1\n",
    "\n",
    "    # Split data into training and test sets\n",
    "    num_points = len(ts)\n",
    "    split_idx = int(train_split * num_points)\n",
    "    train_ts, test_ts = ts[:split_idx], ts[split_idx:]\n",
    "    train_xs, test_xs = xs[:split_idx], xs[split_idx:]\n",
    "    train_ys, test_ys = ys[:split_idx], ys[split_idx:]\n",
    "\n",
    "    return (train_ts, train_xs, train_ys), (test_ts, test_xs, test_ys)\n",
    "\n",
    "\n",
    "def dataloader(ts, xs, ys, batch_size, seq_length, key):\n",
    "    \"\"\"\n",
    "    Dataloader for sampling batches of time series.\n",
    "\n",
    "    ts: Timestamps for all series\n",
    "    xs: Corresponding feature values for all series\n",
    "    ys: Corresponding target values for all series\n",
    "    batch_size: Number of time series in each batch\n",
    "    seq_length: Length of each time series in the batch\n",
    "    key: PRNGKey for random number generation\n",
    "    \"\"\"\n",
    "    dataset_size = len(ts)\n",
    "    indices = jnp.arange(dataset_size - seq_length)  # Indices where sequences of seq_length can start\n",
    "\n",
    "    while True:\n",
    "        perm = jr.choice(key, indices, shape=(batch_size,))  # Randomly sample start indices\n",
    "        (key,) = jr.split(key, 1)\n",
    "        ts_batch = []\n",
    "        xs_batch = []\n",
    "        ys_batch = []\n",
    "\n",
    "        for idx in perm:\n",
    "            ts_batch.append(ts[idx:idx + seq_length])\n",
    "            xs_batch.append(xs[idx:idx + seq_length])\n",
    "            ys_batch.append(ys[idx:idx + seq_length])\n",
    "\n",
    "        yield jnp.stack(ts_batch), jnp.stack(xs_batch), jnp.stack(ys_batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, optimizer, steps, batch_size, seq_length, key):\n",
    "    # Initialize optimizer state\n",
    "    opt_state = optimizer.init(eqx.filter(model, eqx.is_inexact_array))\n",
    "    \n",
    "    (train_ts, train_xs, train_ys) = train_data  # Unpack train and test data\n",
    "    train_key = key\n",
    "\n",
    "    # Create data loader for training data\n",
    "    loader = dataloader(train_ts, train_xs, train_ys, batch_size, seq_length, key=train_key)\n",
    "\n",
    "    @eqx.filter_value_and_grad\n",
    "    def loss(model, ts_i, xs_i, ys_i, key_i):\n",
    "        batch_size, _ = ts_i.shape\n",
    "        key_i = jr.split(key_i, batch_size)\n",
    "        loss = jax.vmap(model.compute_loss)(ts_i, xs_i, ys_i, key=key_i)  # Updated to include xs\n",
    "        return jnp.mean(loss)\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def make_step(model, opt_state, ts_i, xs_i, ys_i, key_i):\n",
    "        value, grads = loss(model, ts_i, xs_i, ys_i, key_i)\n",
    "        key_i = jr.split(key_i, 1)[0]\n",
    "        updates, opt_state = optimizer.update(grads, opt_state)\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        return value, model, opt_state, key_i\n",
    "\n",
    "    for step, (ts_i, xs_i, ys_i) in zip(\n",
    "        range(steps), loader  # Use the data loader directly\n",
    "    ):\n",
    "        opt_key, subkey = jr.split(train_key)\n",
    "        loss_value, model, opt_state, key_i = make_step(model, opt_state, ts_i, xs_i, ys_i, subkey)\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Step {step}, Loss: {loss_value:.4f}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "seq_length = 100  # Length of each time series\n",
    "steps = 1000\n",
    "latent_size = 5\n",
    "x_size = 2  # Number of features (bid_price and offer_price)\n",
    "\n",
    "# Define model\n",
    "func = eqx.nn.MLP(in_size=latent_size, out_size=latent_size, width_size=32, depth=2, key=jax.random.PRNGKey(0))\n",
    "rnn_cell = eqx.nn.GRUCell(x_size, latent_size, key=jax.random.PRNGKey(1))\n",
    "latent_to_y = eqx.nn.MLP(in_size=latent_size, out_size=1, width_size=32, depth=2, key=jax.random.PRNGKey(2))  # Update output size to 1\n",
    "\n",
    "model = NeuralODEModel(func, rnn_cell, latent_to_y, latent_size, x_size)\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer = optax.adam(learning_rate=1e-3)\n",
    "\n",
    "# Get data, split into training and test\n",
    "data = get_data(train_split=0.8)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot interpret value of type <class 'jax._src.custom_derivatives.custom_jvp'> as an abstract array; it does not have a dtype attribute",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/js/9hpr3tt17xd6ttv9xy3zrhw80000gn/T/ipykernel_35533/513495714.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPRNGKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/js/9hpr3tt17xd6ttv9xy3zrhw80000gn/T/ipykernel_35533/1066946343.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_data, optimizer, steps, batch_size, seq_length, key)\u001b[0m\n\u001b[1;32m     28\u001b[0m     ):\n\u001b[1;32m     29\u001b[0m         \u001b[0mopt_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mts_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "    \u001b[0;31m[... skipping hidden 15 frame]\u001b[0m\n",
      "\u001b[0;32m/var/folders/js/9hpr3tt17xd6ttv9xy3zrhw80000gn/T/ipykernel_35533/1066946343.py\u001b[0m in \u001b[0;36mmake_step\u001b[0;34m(model, opt_state, ts_i, xs_i, ys_i, key_i)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0meqx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_jit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmake_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mts_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mts_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mkey_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "    \u001b[0;31m[... skipping hidden 10 frame]\u001b[0m\n",
      "\u001b[0;32m/var/folders/js/9hpr3tt17xd6ttv9xy3zrhw80000gn/T/ipykernel_35533/1066946343.py\u001b[0m in \u001b[0;36mloss\u001b[0;34m(model, ts_i, xs_i, ys_i, key_i)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mts_i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mkey_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_i\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Updated to include xs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "    \u001b[0;31m[... skipping hidden 8 frame]\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/jax/_src/api_util.py\u001b[0m in \u001b[0;36m_shaped_abstractify_slow\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    575\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanonicalize_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_extended_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     raise TypeError(\n\u001b[0m\u001b[1;32m    578\u001b[0m         \u001b[0;34mf\"Cannot interpret value of type {type(x)} as an abstract array; it \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m         \"does not have a dtype attribute\")\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot interpret value of type <class 'jax._src.custom_derivatives.custom_jvp'> as an abstract array; it does not have a dtype attribute"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "trained_model = train(model, train_data, optimizer, steps=steps, batch_size=batch_size, seq_length=seq_length, key=jax.random.PRNGKey(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
